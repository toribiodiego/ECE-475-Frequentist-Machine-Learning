{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf1faFkWppfO"
      },
      "source": [
        "Diego Toribio <br>\n",
        "Professor Sam Keene <br>\n",
        "Frequentist Machine Learning <br>\n",
        "Project 4: Non-Negative Matrix Factorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fr6VyUUxvN6b",
        "outputId": "c70772ae-f947-4218-cb2f-2ab5914c0f23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-surprise\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.13.1)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp310-cp310-linux_x86_64.whl size=2357281 sha256=25ddeb63664fee57450a865eb6d6b52e8463d19e2926a697c10e674ce010317e\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/3f/df/6acbf0a40397d9bf3ff97f582cc22fb9ce66adde75bc71fd54\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise\n",
            "Successfully installed scikit-surprise-1.1.4\n",
            "--2024-12-04 14:42:03--  https://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
            "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
            "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4924029 (4.7M) [application/zip]\n",
            "Saving to: ‘ml-100k.zip’\n",
            "\n",
            "ml-100k.zip         100%[===================>]   4.70M  8.39MB/s    in 0.6s    \n",
            "\n",
            "2024-12-04 14:42:04 (8.39 MB/s) - ‘ml-100k.zip’ saved [4924029/4924029]\n",
            "\n",
            "Archive:  ml-100k.zip\n",
            "   creating: ml-100k/\n",
            "  inflating: ml-100k/allbut.pl       \n",
            "  inflating: ml-100k/mku.sh          \n",
            "  inflating: ml-100k/README          \n",
            "  inflating: ml-100k/u.data          \n",
            "  inflating: ml-100k/u.genre         \n",
            "  inflating: ml-100k/u.info          \n",
            "  inflating: ml-100k/u.item          \n",
            "  inflating: ml-100k/u.occupation    \n",
            "  inflating: ml-100k/u.user          \n",
            "  inflating: ml-100k/u1.base         \n",
            "  inflating: ml-100k/u1.test         \n",
            "  inflating: ml-100k/u2.base         \n",
            "  inflating: ml-100k/u2.test         \n",
            "  inflating: ml-100k/u3.base         \n",
            "  inflating: ml-100k/u3.test         \n",
            "  inflating: ml-100k/u4.base         \n",
            "  inflating: ml-100k/u4.test         \n",
            "  inflating: ml-100k/u5.base         \n",
            "  inflating: ml-100k/u5.test         \n",
            "  inflating: ml-100k/ua.base         \n",
            "  inflating: ml-100k/ua.test         \n",
            "  inflating: ml-100k/ub.base         \n",
            "  inflating: ml-100k/ub.test         \n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-surprise\n",
        "\n",
        "!wget -nc https://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
        "!unzip -n ml-100k.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "from sklearn.model_selection import KFold\n",
        "from surprise import Dataset, Reader, NMF\n",
        "from surprise.model_selection import GridSearchCV, train_test_split"
      ],
      "metadata": {
        "id": "MpaOZNlEU0z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNa-fDqHpkQY",
        "outputId": "05e6aaac-4b38-40d7-f0fc-c3aceb576338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   25.4s\n",
            "[Parallel(n_jobs=-1)]: Done 162 out of 162 | elapsed:  1.6min finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best RMSE score: 0.9582954189422881\n",
            "Best parameters: {'n_factors': 60, 'n_epochs': 10, 'reg_pu': 1.5, 'reg_qi': 0.05}\n",
            "\n",
            "Top 10 evaluation predictions for user 13:\n",
            "+-------+--------+---------------+------------------+----------------------------------+\n",
            "|       | UserID | Actual Rating | Estimated Rating |           movie_title            |\n",
            "+-------+--------+---------------+------------------+----------------------------------+\n",
            "|  302  |   13   |      5.0      |      4.133       | Shawshank Redemption, The (1994) |\n",
            "| 1145  |   13   |      5.0      |      4.082       |      Godfather, The (1972)       |\n",
            "| 2563  |   13   |      2.0      |      3.941       |         Boot, Das (1981)         |\n",
            "| 3903  |   13   |      4.0      |      3.923       | Manchurian Candidate, The (1962) |\n",
            "| 13623 |   13   |      5.0      |      3.887       |    Lawrence of Arabia (1962)     |\n",
            "| 13801 |   13   |      4.0      |      3.879       |     Great Escape, The (1963)     |\n",
            "| 15106 |   13   |      2.0      |      3.815       |    Princess Bride, The (1987)    |\n",
            "|  80   |   13   |      5.0      |      3.813       |   It Happened One Night (1934)   |\n",
            "| 18587 |   13   |      5.0      |       3.81       |           Glory (1989)           |\n",
            "| 17188 |   13   |      4.0      |      3.795       |  Philadelphia Story, The (1940)  |\n",
            "+-------+--------+---------------+------------------+----------------------------------+\n"
          ]
        }
      ],
      "source": [
        "class DataLoader:\n",
        "    def __init__(self, ratings_file, items_file):\n",
        "        self.ratings_file = ratings_file\n",
        "        self.items_file = items_file\n",
        "        self.ratings_df = None\n",
        "        self.items_df = None\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load ratings and movie metadata.\"\"\"\n",
        "        ratings_columns = [\"user_id\", \"item_id\", \"rating\", \"timestamp\"]\n",
        "        item_columns = [\n",
        "            \"movie_id\",\n",
        "            \"movie_title\",\n",
        "            \"release_date\",\n",
        "            \"video_release_date\",\n",
        "            \"IMDb_URL\",\n",
        "            \"unknown\",\n",
        "            \"Action\",\n",
        "            \"Adventure\",\n",
        "            \"Animation\",\n",
        "            \"Children's\",\n",
        "            \"Comedy\",\n",
        "            \"Crime\",\n",
        "            \"Documentary\",\n",
        "            \"Drama\",\n",
        "            \"Fantasy\",\n",
        "            \"Film-Noir\",\n",
        "            \"Horror\",\n",
        "            \"Musical\",\n",
        "            \"Mystery\",\n",
        "            \"Romance\",\n",
        "            \"Sci-Fi\",\n",
        "            \"Thriller\",\n",
        "            \"War\",\n",
        "            \"Western\",\n",
        "        ]\n",
        "\n",
        "        self.ratings_df = pd.read_csv(\n",
        "            self.ratings_file, sep=\"\\t\", names=ratings_columns, encoding='latin-1'\n",
        "        )\n",
        "        self.items_df = pd.read_csv(\n",
        "            self.items_file,\n",
        "            sep=\"|\",\n",
        "            encoding=\"latin-1\",\n",
        "            header=None,\n",
        "            names=item_columns,\n",
        "            usecols=[\"movie_id\", \"movie_title\"],\n",
        "        )\n",
        "\n",
        "    def get_ratings_df(self):\n",
        "        return self.ratings_df\n",
        "\n",
        "    def get_items_df(self):\n",
        "        return self.items_df\n",
        "\n",
        "\n",
        "class MovieRecommender:\n",
        "    def __init__(self, data_loader):\n",
        "        self.data_loader = data_loader\n",
        "        self.ratings_df = None\n",
        "        self.items_df = None\n",
        "        self.data = None\n",
        "        self.best_algo = None\n",
        "        self.predictions = None\n",
        "        self.merged_df = None\n",
        "        self.trainset = None\n",
        "        self.testset = None\n",
        "\n",
        "    def load_data(self):\n",
        "        self.data_loader.load_data()\n",
        "        self.ratings_df = self.data_loader.get_ratings_df()\n",
        "        self.items_df = self.data_loader.get_items_df()\n",
        "\n",
        "        reader = Reader(rating_scale=(1, 5))\n",
        "        self.data = Dataset.load_from_df(\n",
        "            self.ratings_df[[\"user_id\", \"item_id\", \"rating\"]], reader\n",
        "        )\n",
        "\n",
        "    def train_model(self, param_grid):\n",
        "        grid = GridSearchCV(\n",
        "            NMF,\n",
        "            param_grid=param_grid,\n",
        "            measures=[\"rmse\"],\n",
        "            cv=2,\n",
        "            n_jobs=-1,\n",
        "            joblib_verbose=1,\n",
        "        )\n",
        "        grid.fit(self.data)\n",
        "        print(f\"Best RMSE score: {grid.best_score['rmse']}\")\n",
        "        print(\"Best parameters:\", grid.best_params[\"rmse\"])\n",
        "        self.best_algo = grid.best_estimator[\"rmse\"]\n",
        "\n",
        "    def generate_predictions(self, test_size=0.2, random_state=42):\n",
        "        trainset, self.testset = train_test_split(self.data, test_size=test_size, random_state=random_state)\n",
        "        self.best_algo.fit(trainset)\n",
        "        self.predictions = self.best_algo.test(self.testset)\n",
        "\n",
        "        pred_df = pd.DataFrame(\n",
        "            [\n",
        "                {\n",
        "                    \"UserID\": int(pred.uid),\n",
        "                    \"ItemID\": int(pred.iid),\n",
        "                    \"Estimated Rating\": round(pred.est, 3),\n",
        "                    \"Actual Rating\": pred.r_ui,\n",
        "                }\n",
        "                for pred in self.predictions\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.merged_df = pd.merge(\n",
        "            pred_df,\n",
        "            self.items_df,\n",
        "            left_on=\"ItemID\",\n",
        "            right_on=\"movie_id\",\n",
        "            how=\"left\",\n",
        "        )\n",
        "        self.merged_df.drop([\"movie_id\"], axis=1, inplace=True)\n",
        "\n",
        "    def generate_recommendations(self, user_id, n=10):\n",
        "        trainset = self.data.build_full_trainset()\n",
        "        self.best_algo.fit(trainset)\n",
        "\n",
        "        anti_testset = trainset.build_anti_testset()\n",
        "\n",
        "        user_anti_testset = [pair for pair in anti_testset if pair[0] == str(user_id)]\n",
        "\n",
        "        predictions = self.best_algo.test(user_anti_testset)\n",
        "\n",
        "        rec_df = pd.DataFrame(\n",
        "            [\n",
        "                {\n",
        "                    \"UserID\": int(pred.uid),\n",
        "                    \"ItemID\": int(pred.iid),\n",
        "                    \"Estimated Rating\": round(pred.est, 3),\n",
        "                }\n",
        "                for pred in predictions\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        merged_rec_df = pd.merge(\n",
        "            rec_df,\n",
        "            self.items_df,\n",
        "            left_on=\"ItemID\",\n",
        "            right_on=\"movie_id\",\n",
        "            how=\"left\",\n",
        "        )\n",
        "        merged_rec_df.drop([\"movie_id\"], axis=1, inplace=True)\n",
        "\n",
        "        top_n = merged_rec_df.sort_values(\"Estimated Rating\", ascending=False).head(n)\n",
        "        return top_n[[\"UserID\", \"Estimated Rating\", \"movie_title\"]]\n",
        "\n",
        "    def get_top_n_recommendations_evaluation(self, user_id, n=10):\n",
        "        if self.merged_df is None:\n",
        "            raise ValueError(\n",
        "                \"Predictions have not been generated. Call generate_predictions() first.\"\n",
        "            )\n",
        "\n",
        "        user_predictions = self.merged_df[self.merged_df[\"UserID\"] == user_id]\n",
        "        user_predictions = user_predictions.sort_values(\"Estimated Rating\", ascending=False)\n",
        "        return user_predictions.head(n)[[\"UserID\", \"Actual Rating\", \"Estimated Rating\", \"movie_title\"]]\n",
        "\n",
        "\n",
        "config = {\n",
        "    \"n_factors\": [40, 50, 60],\n",
        "    \"n_epochs\": [8, 10, 12],\n",
        "    \"reg_pu\": [0.5, 1.0, 1.5],\n",
        "    \"reg_qi\": [0.05, 0.1, 0.15],\n",
        "}\n",
        "\n",
        "ratings_file = \"ml-100k/u.data\"\n",
        "items_file = \"ml-100k/u.item\"\n",
        "\n",
        "data_loader = DataLoader(ratings_file, items_file)\n",
        "recommender = MovieRecommender(data_loader)\n",
        "\n",
        "recommender.load_data()\n",
        "recommender.train_model(config)\n",
        "recommender.generate_predictions()\n",
        "\n",
        "# Get top N predictions from the test set\n",
        "user_id_eval = 13\n",
        "top_evaluation = recommender.get_top_n_recommendations_evaluation(user_id_eval, n=10)\n",
        "\n",
        "print(f\"\\nTop 10 evaluation predictions for user {user_id_eval}:\")\n",
        "print(tabulate(top_evaluation, headers='keys', tablefmt='pretty'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4NlOYSMp44s"
      },
      "source": [
        "## Stretch Goal \\#1\n",
        "\n",
        "Using an explicit feedback dataset, implement the NMF algorithm by hand, tune it via cross validation to select the # of latent dims and regularization parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCotk6rMp30Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18764c6a-4ee9-4500-8562-988da1d1adfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting cross-validation...\n",
            "num_factors: 14, beta: 0.01, Avg RMSE: 1.6941\n",
            "num_factors: 14, beta: 0.05, Avg RMSE: 1.5533\n",
            "num_factors: 14, beta: 0.1, Avg RMSE: 1.4944\n",
            "num_factors: 15, beta: 0.01, Avg RMSE: 1.5927\n",
            "num_factors: 15, beta: 0.05, Avg RMSE: 1.6150\n",
            "num_factors: 15, beta: 0.1, Avg RMSE: 1.5643\n",
            "num_factors: 16, beta: 0.01, Avg RMSE: 1.5910\n",
            "num_factors: 16, beta: 0.05, Avg RMSE: 1.6709\n",
            "num_factors: 16, beta: 0.1, Avg RMSE: 1.6312\n",
            "\n",
            "Best RMSE: 1.4944\n",
            "Best Parameters: {'num_factors': 14, 'beta': 0.1}\n",
            "\n",
            "Top 10 evaluation predictions for user 13:\n",
            "+----+--------+---------------+------------------+----------------------------------------+\n",
            "|    | UserID | Actual Rating | Estimated Rating |              movie_title               |\n",
            "+----+--------+---------------+------------------+----------------------------------------+\n",
            "| 30 |   13   |      5.0      |       5.0        |     Raiders of the Lost Ark (1981)     |\n",
            "| 29 |   13   |      4.0      |       5.0        |       Princess Bride, The (1987)       |\n",
            "| 27 |   13   |      4.0      |       5.0        | Monty Python and the Holy Grail (1974) |\n",
            "| 23 |   13   |      3.0      |       5.0        |     Independence Day (ID4) (1996)      |\n",
            "| 36 |   13   |      5.0      |       5.0        |         Terminator, The (1984)         |\n",
            "| 38 |   13   |      5.0      |       5.0        |       Back to the Future (1985)        |\n",
            "| 8  |   13   |      3.0      |      4.878       |           Braveheart (1995)            |\n",
            "| 35 |   13   |      4.0      |      4.792       |             Amadeus (1984)             |\n",
            "| 32 |   13   |      1.0      |      4.554       |             Aliens (1986)              |\n",
            "| 2  |   13   |      5.0      |      4.189       |       Usual Suspects, The (1995)       |\n",
            "+----+--------+---------------+------------------+----------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "class NMF_SGD:\n",
        "    def __init__(\n",
        "        self, R, num_factors=10, alpha=0.001, beta=0.02, epochs=20, random_state=None\n",
        "    ):\n",
        "        self.R = R\n",
        "        self.num_users, self.num_items = R.shape\n",
        "        self.num_factors = num_factors\n",
        "        self.alpha = alpha\n",
        "        self.reg_pu = beta\n",
        "        self.reg_qi = beta\n",
        "        self.epochs = epochs\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self):\n",
        "        np.random.seed(self.random_state)\n",
        "        self.P = np.random.rand(self.num_users, self.num_factors)\n",
        "        self.Q = np.random.rand(self.num_items, self.num_factors)\n",
        "\n",
        "        rows, cols = self.R.nonzero()\n",
        "        ratings = self.R[rows, cols]\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            predictions = np.sum(self.P[rows, :] * self.Q[cols, :], axis=1)\n",
        "            errors = ratings - predictions\n",
        "\n",
        "            dp = -2 * errors[:, np.newaxis] * self.Q[cols, :] + self.reg_pu * self.P[rows, :]\n",
        "            dq = -2 * errors[:, np.newaxis] * self.P[rows, :] + self.reg_qi * self.Q[cols, :]\n",
        "\n",
        "            P_grad = np.zeros_like(self.P)\n",
        "            Q_grad = np.zeros_like(self.Q)\n",
        "\n",
        "            np.add.at(P_grad, rows, dp)\n",
        "            np.add.at(Q_grad, cols, dq)\n",
        "\n",
        "            self.P -= self.alpha * P_grad\n",
        "            self.Q -= self.alpha * Q_grad\n",
        "\n",
        "            self.P = np.maximum(self.P, 0)\n",
        "            self.Q = np.maximum(self.Q, 0)\n",
        "\n",
        "    def predict_single(self, i, j):\n",
        "        return np.dot(self.P[i, :], self.Q[j, :].T)\n",
        "\n",
        "    def predict_all(self):\n",
        "        return np.dot(self.P, self.Q.T)\n",
        "\n",
        "\n",
        "def cross_validate_nmf(\n",
        "    R, num_factors_list, beta_list, alpha=0.001, epochs=20, n_splits=3\n",
        "):\n",
        "    best_rmse = float(\"inf\")\n",
        "    best_params = {\"num_factors\": None, \"beta\": None}\n",
        "\n",
        "    print(\"Starting cross-validation...\")\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    for num_factors in num_factors_list:\n",
        "        for beta in beta_list:\n",
        "            rmses = []\n",
        "            for train_indices, test_indices in kf.split(R):\n",
        "                R_train = np.copy(R)\n",
        "                R_test = np.zeros(R.shape)\n",
        "                for idx in test_indices:\n",
        "                    R_train[idx, :] = 0\n",
        "                    R_test[idx, :] = R[idx, :]\n",
        "                nmf = NMF_SGD(\n",
        "                    R_train,\n",
        "                    num_factors=num_factors,\n",
        "                    alpha=alpha,\n",
        "                    beta=beta,\n",
        "                    epochs=epochs,\n",
        "                )\n",
        "                nmf.fit()\n",
        "                predicted = nmf.predict_all()\n",
        "                xs, ys = R_test.nonzero()\n",
        "                error = 0\n",
        "                count = 0\n",
        "                for x, y in zip(xs, ys):\n",
        "                    error += (R_test[x, y] - predicted[x, y]) ** 2\n",
        "                    count += 1\n",
        "                rmse = np.sqrt(error / count) if count > 0 else float(\"inf\")\n",
        "                rmses.append(rmse)\n",
        "            avg_rmse = np.mean(rmses)\n",
        "            print(f\"num_factors: {num_factors}, beta: {beta}, Avg RMSE: {avg_rmse:.4f}\")\n",
        "            if avg_rmse < best_rmse:\n",
        "                best_rmse = avg_rmse\n",
        "                best_params = {\"num_factors\": num_factors, \"beta\": beta}\n",
        "\n",
        "    return best_params, best_rmse\n",
        "\n",
        "\n",
        "class NMFRecommender:\n",
        "    def __init__(self, R, items_df):\n",
        "        self.R = R\n",
        "        self.items_df = items_df\n",
        "        self.nmf_model = None\n",
        "        self.R_pred = None\n",
        "        self.R_pred_df = None\n",
        "        self.R_df = pd.DataFrame(R)\n",
        "\n",
        "    def cross_validate(\n",
        "        self, num_factors_list, beta_list, alpha=0.001, epochs=20, n_splits=3\n",
        "    ):\n",
        "        return cross_validate_nmf(\n",
        "            self.R, num_factors_list, beta_list, alpha, epochs, n_splits\n",
        "        )\n",
        "\n",
        "    def train(self, num_factors, beta, alpha=0.001, epochs=20, random_state=None):\n",
        "        self.nmf_model = NMF_SGD(\n",
        "            self.R, num_factors=num_factors, beta=beta, alpha=alpha, epochs=epochs, random_state=random_state\n",
        "        )\n",
        "        self.nmf_model.fit()\n",
        "        self.R_pred = self.nmf_model.predict_all()\n",
        "        self.R_pred_df = pd.DataFrame(\n",
        "            self.R_pred, index=self.R_df.index, columns=self.R_df.columns\n",
        "        )\n",
        "\n",
        "    def get_top_n_recommendations(self, user_id, n=10):\n",
        "        user_ratings = self.R_df.loc[user_id]\n",
        "        user_predictions = self.R_pred_df.loc[user_id]\n",
        "        unrated_items = user_ratings[user_ratings == 0].index\n",
        "        predicted_ratings = user_predictions[unrated_items]\n",
        "        top_n_items = predicted_ratings.sort_values(ascending=False).head(n).index\n",
        "        top_n_titles = self.items_df[self.items_df[\"movie_id\"].isin(top_n_items)][\"movie_title\"]\n",
        "        return pd.DataFrame(\n",
        "            {\n",
        "                \"movie_id\": top_n_items,\n",
        "                \"predicted_rating\": predicted_ratings[top_n_items].values,\n",
        "                \"movie_title\": top_n_titles.values,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    def get_top_n_recommendations_evaluation(self, user_id, n=10):\n",
        "        user_ratings = self.R_df.loc[user_id]\n",
        "        user_predictions = self.R_pred_df.loc[user_id]\n",
        "\n",
        "        rated_items = user_ratings[user_ratings > 0].index\n",
        "        user_eval_df = pd.DataFrame({\n",
        "            \"UserID\": user_id,\n",
        "            \"Actual Rating\": user_ratings[rated_items].values,\n",
        "            \"Estimated Rating\": user_predictions[rated_items].values,\n",
        "            \"movie_id\": rated_items\n",
        "        })\n",
        "\n",
        "        user_eval_df[\"Estimated Rating\"] = np.round(user_eval_df[\"Estimated Rating\"], 3)\n",
        "        user_eval_df[\"Estimated Rating\"] = np.minimum(user_eval_df[\"Estimated Rating\"], 5.0)\n",
        "\n",
        "        user_eval_df = user_eval_df.merge(self.items_df, how=\"left\", on=\"movie_id\")\n",
        "        user_eval_df = user_eval_df.sort_values(\"Estimated Rating\", ascending=False).head(n)\n",
        "\n",
        "        return user_eval_df[[\"UserID\", \"Actual Rating\", \"Estimated Rating\", \"movie_title\"]]\n",
        "\n",
        "\n",
        "# Load your data\n",
        "ratings_file = \"ml-100k/u.data\"\n",
        "items_file = \"ml-100k/u.item\"\n",
        "\n",
        "data_loader = DataLoader(ratings_file, items_file)\n",
        "data_loader.load_data()\n",
        "ratings_df = data_loader.get_ratings_df()\n",
        "items_df = data_loader.get_items_df()\n",
        "\n",
        "ratings_df[\"user_id\"] -= 1\n",
        "ratings_df[\"item_id\"] -= 1\n",
        "items_df[\"movie_id\"] -= 1\n",
        "\n",
        "num_users = ratings_df[\"user_id\"].nunique()\n",
        "num_items = ratings_df[\"item_id\"].nunique()\n",
        "R_df = ratings_df.pivot(index=\"user_id\", columns=\"item_id\", values=\"rating\").fillna(0)\n",
        "R = R_df.values\n",
        "\n",
        "nmf_recommender = NMFRecommender(R, items_df)\n",
        "\n",
        "num_factors_list = [14, 15, 16]\n",
        "beta_list = [0.01, 0.05, 0.1]\n",
        "\n",
        "best_params, best_rmse = nmf_recommender.cross_validate(\n",
        "    num_factors_list, beta_list, alpha=0.001, epochs=20, n_splits=3\n",
        ")\n",
        "\n",
        "nmf_recommender.train(\n",
        "    num_factors=best_params[\"num_factors\"],\n",
        "    beta=best_params[\"beta\"],\n",
        "    alpha=0.001,\n",
        "    epochs=20,\n",
        "    random_state=13,\n",
        ")\n",
        "\n",
        "# Get top N evaluation predictions for user 13\n",
        "user_id = 13\n",
        "top_evaluation = nmf_recommender.get_top_n_recommendations_evaluation(user_id, n=10)\n",
        "\n",
        "print(f\"\\nBest RMSE: {best_rmse:.4f}\")\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"\\nTop 10 evaluation predictions for user {user_id}:\")\n",
        "print(tabulate(top_evaluation, headers='keys', tablefmt='pretty'))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}